
# An Overview of DPO

DPO is covered relatively well online and there are a couple resources I used while working on this project. That being said, it still took a bit to get comfortable with the details. This document is an amalgamation of my personal notes I took when first diving in, and hopefully it can be helpful for someone else! (That being said, if you notice anything incorrect here, feel free to reach out and let me know so I can update it :-).)

[original DPO paper](https://arxiv.org/pdf/2305.18290). It takes a couple reads to get what's fully going on, but I highly recommend it; it has all the information you need (including sample code and an additional derivation for the loss for listwise data in the appendix).

## Preliminaries

### DPO vs RLHF

DPO was developed as an efficient alternative to RLHF. Briefly, RLHF consists of the following steps:

1. Initialize two copies of a LM: the reference model and the policy model (the latter of which will be updated; the reference model weights will not change). The initialized version of the model can be supervisedly fine-tuned (SFT).
2. Provide prompts (x) to the LM, and collect two generations to the given prompt (y1 | x, y2 | x). Offline, collect human preferences between the two generations y1 and y2; we can say that these preferences are generated by an unknown reward model r(y,x). (Think about r(y,x) has a model that humans possess internally to assign some value or reward to the generations y given x; if y1 has a higher reward than y2, a human prefers y1.)
3. So now, we have access to a dataset of prompts, their corresponding generations, and the preferences assigned to those generations. This dataset is used to train a concrete reward model––the structure of this model is flexible, but the DPO authors point out that it's typically a copy of the LM with an additional linear layer on top that can output a scalar value.
4. With this learned reward model, we can use some new prompts, collect generations for them, evaluate using the learned reward model, and use the reward to inform the chosen algorithm for updating the model weights (e.g. the PPO algorithm). I'll also note that the reward itself isn't directly used; the value takes into account both the reward as well as the divergence from the reference model (to discourage the policy LM from being drastically different to the reference LM by the end of training).
5. The policy LM, now aligned to human preferences, can be used for downstream tasks.

The main difference between RLHF and DPO is the development and use of the reward model. Obviously, maintaining and training a whole separate model in addition to the reference and policy model is going to take up storage and compute resources. The DPO authors show that through some re-parametrization, the human preference model can be rewritten equivalently in terms of just the policy and reference model (no need for the reward model!). 

Let's review the steps for DPO (note that model initialization and data collection are identical to the RLHF set-up):

1. *(same as RLHF Step 1)
2. *(same as RLHF Step 2)
3. Keeping the DPO loss function in mind (eq. 7 from the DPO paper), we'll compute the log likelihoods of (a) the policy model on the (a.1.) preferred and (a.2.) dispreferred generations provided prompt x; and (b) the reference model on the (b.1.) preferred and (b.2.) dispreferred generations provided prompt x. In practice, we're grabbing the logits of (y|x) on the given model by completing a forward pass. 
4. With the logits, we can compute the DPO loss and throw it through backprop. The loss is easier to understand looking at sample code (again, provided in the DPO paper's appendix). 
5. *(same as RLHF Step 5) :-)

### Listwise Preferences








